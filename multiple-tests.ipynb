{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "config = {\n",
    "    # config for the neural network\n",
    "    \"nn\": {\n",
    "        \"window\": 90, # this is also the window, window size or entries used to predict the future price\n",
    "        \"nLayers\": 5, # 2\n",
    "        \"neurons\": 64, # 256\n",
    "        \"nFeatures\": 5,\n",
    "        \"dropout\": 0.3,\n",
    "        \"lossFunc\": \"huber_loss\", # \"huber_loss\" Huber looss is better to give outliers some weight but not too much, which can be good for predicting asset prices?\n",
    "        \"optimizer\": \"adam\", # \"adam\"\n",
    "        \"layerType\": LSTM, # could set it upt to be a list of the layers that need to be generated [LSTM LSTM] 2 hidden layers \n",
    "    },\n",
    "    \"data\": {\n",
    "        \"ticker\": \"AMZN\", # \"TSLA\" \"AMZN\" \"AAPL\" \"NVDA\" \"AMD\"\n",
    "        \"offset\": pd.DateOffset(years = 3, months = 0, days = 0),\n",
    "        \"lookahead\": 7,\n",
    "        \"validation\": 0.1, # percentage of training data to be used as validation\n",
    "        \"testSize\": 0.1, # to test on \n",
    "        \"scale\": True,\n",
    "        \"scaler\": preprocessing.MinMaxScaler(),\n",
    "        \"inputs\": ['adjclose', 'volume', 'open', 'high', 'low'],\n",
    "    },\n",
    "    \"plots\": {\n",
    "        \"show_plots\": True,\n",
    "        \"xticks_interval\": 90,\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\", # \"cuda\" or \"cpu\"\n",
    "        \"batch_size\": 64, # 64\n",
    "        \"num_epoch\": 50, # 500\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}\n",
    "# DISPLAY AROUGH ESTIMATE OF HOW MANY DAYS WILL BE USED FOR EACH SECTION\n",
    "damping = 0.9 # some extra damping for extra days that data isn't collected\n",
    "\n",
    "offset = config[\"data\"][\"offset\"]\n",
    "ts1 = pd.Timestamp('2023-02-25')\n",
    "ts2 = ts1 + offset\n",
    "td = ts2 - ts1\n",
    "days_offset = int(td.days/7*5*damping) # find out the roungh amount of days imported\n",
    "print(f\"{days_offset} Total days used will be roughly imported\\n\")\n",
    "print(f\"{days_offset* (1-config['data']['validation']-config['data']['testSize'])} Days used for training\")\n",
    "print(f\"{days_offset*config['data']['validation']} Days used for validation\")\n",
    "print(f\"{days_offset*config['data']['testSize']} Days used for testing\")\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dropout, LSTM, Dense\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "HERE mean_absolute_error CAN BE REPLACED BY mse?\n",
    "Also need to have a look at bidirectional network and how that works and so forth\n",
    "\n",
    "\"\"\"\n",
    "def generateModel(\n",
    "    window,\n",
    "    n_layers=2,\n",
    "    neurons=256,\n",
    "    n_features=5,\n",
    "    dropout=0.3,\n",
    "    loss=\"mean_absolute_error\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    layerType=LSTM\n",
    "    ):\n",
    "    # generate a keras model to linearly group a stasck of layers\n",
    "    model = models.Sequential()\n",
    "    # repeat for each layer\n",
    "    for layer in range(n_layers):\n",
    "        # check for first layer\n",
    "        if layer == 0:\n",
    "            \"\"\"NEED TO FIGURE OUT THE DIFFERENE BETWEEN THEESE TWO LINES OF CODE, INPUT SHAPE AND BATCH INPUT SHAPE\"\"\"\n",
    "            model.add(LSTM(neurons, return_sequences=True, input_shape=(window, n_features)))\n",
    "            # model.add(layerType(neurons, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        # check for final layer\n",
    "        elif layer == n_layers - 1:\n",
    "            model.add(layerType(neurons, return_sequences=False))\n",
    "        # middle/hidden layers\n",
    "        else:\n",
    "            model.add(layerType(neurons, return_sequences=True))\n",
    "\n",
    "        # add dropout after each layer\n",
    "        \"\"\" this layer helps prevent over fitting\n",
    "        The Dropout layer randomly sets input units to 0\n",
    "        with a frequency of rate at each step during training time,\n",
    "        which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate)\n",
    "        such that the sum over all inputs is unchanged.\"\"\"\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    # compile the model once it has be compelted\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Import yfinance package (NEED OT DOUBLE CHEKC IF THIS IS ACTUALLY A MODEL THAT CAN BE USED)\n",
    "\n",
    "# import yfinance as yf # make sure to unisntall if this isn't used in the future\n",
    "from yahoo_fin import stock_info\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def getData(\n",
    "    ticker,\n",
    "    offset\n",
    "    ):\n",
    "    # setup the start date and end date\n",
    "    startDate = (datetime.now() - offset).date()\n",
    "    endDate = datetime.now().date()\n",
    "    # Get the data\n",
    "    stocks = stock_info.get_data(ticker, startDate, endDate)\n",
    "    # display how many days of data are being used for this\n",
    "    print(f\"{ticker} data has been imported between {startDate} and the {endDate}, for a total of {len(stocks)} days of data\")\n",
    "    # print(f\"Each day contains the prices for: {stocks.columns}\")\n",
    "    return stocks\n",
    "\n",
    "def setupData(\n",
    "    stocks, # data to be reshaped\n",
    "    window = 50, # the number of days used to predic the next value (window size or lookback perdiod)\n",
    "    predAhead = 10, # the number of days ahead that you are tyring to predict\n",
    "    validationSize = 0.1, # amount of data out of the training data to be used as validation\n",
    "    testSize = 0.2, # the amount of data that will be  used for testing e.g. 0.2 -> 20%\n",
    "    scale = True, # minmaxscaler\n",
    "    scaler = preprocessing.MinMaxScaler(), # the scaler to be used\n",
    "    inputs = ['adjclose', 'volume', 'open', 'high', 'low']\n",
    "    ):\n",
    "    # Setup a dictionary to store all the data\n",
    "    data = {}\n",
    "    # copy the data frame\n",
    "    data[\"raw\"] = stocks.copy()\n",
    "\n",
    "    \"\"\"This is something that really needs to be fixed in the future\"\"\"\n",
    "    #Â I HAVE TO COPY THE STOCKS SO THAT THE ORIGINAL ONES AREN'T RETURNED? WHAT IS THIS ABOUT\n",
    "    stocks = stocks.copy()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # should we scale down between 0 and 1? It can make analysis easier\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in inputs:\n",
    "            # scale the columns\n",
    "            stocks[column] = scaler.fit_transform(np.expand_dims(stocks[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        data[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # create input a variable with x and y\n",
    "    x = []\n",
    "    y = []\n",
    "    ydates = []\n",
    "    for i in range(len(stocks)-window-predAhead+1):\n",
    "        x.append(np.array(stocks[inputs].values[i:i+window]))\n",
    "        y.append(stocks[\"adjclose\"].values[i+window+predAhead-1])\n",
    "        ydates.append(stocks.index[i+window+1])\n",
    "    # convert into numpy arrays\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # store x and y into the data dictionary\n",
    "    data[\"x\"] = x\n",
    "    data[\"y\"] = y\n",
    "    data[\"ydates\"] =  ydates\n",
    "\n",
    "    # split the dataset into training & testing sets by date\n",
    "    # make sure to convert to an integer so that it can be used for indexing\n",
    "    trainSamples = int((1 - testSize - validationSize) * len(x))\n",
    "    valSamples = int(validationSize * len(x))\n",
    "    data[\"xtrain\"] = x[:trainSamples]\n",
    "    data[\"ytrain\"] = y[:trainSamples]\n",
    "\n",
    "    data[\"xval\"] = x[trainSamples:trainSamples+valSamples]\n",
    "    data[\"yval\"] = y[trainSamples:trainSamples+valSamples]\n",
    "\n",
    "    data[\"xtest\"]  = x[trainSamples+valSamples:]\n",
    "    data[\"ytest\"]  = y[trainSamples+valSamples:]\n",
    "    \n",
    "    data[\"trainDates\"] = ydates[:trainSamples]\n",
    "    data[\"valDates\"] = ydates[trainSamples:trainSamples+valSamples]\n",
    "    data[\"testDates\"] = ydates[trainSamples+valSamples:]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Dataset settings\n",
    "windows = [90] #[10, 30, 50, 70, 90]\n",
    "tickers =  \"AMZN\".split(\",\") #\"TSLA,AMZN,AAPL,NVDA,AMD\".split(\",\")\n",
    "lookaheads = [2, 7]\n",
    "validations = [0.1]\n",
    "tests = [0.1]\n",
    "scaled = [True]\n",
    "\n",
    "# Comparison options\n",
    "epochs = [100, 300, 500] # [10, 50, 100, 200, 400]\n",
    "layers = [2, 4, 7] # [2, 3, 4, 5]\n",
    "num_neurons = [4, 16, 32] # [16, 32, 64, 256]\n",
    "dropouts = [0.2, 0.3] # [0.2, 0.3, 0.4] # 0.2 to 0.5 the higher the more the network will underfit\n",
    "loss_functions = ['huber_loss'] # 'huber_loss', 'mse', 'mae', 'huber', 'log_cosh', 'mean_squared_logarithmic_error'\n",
    "optimizers = ['adam'] # ['rmsprop', 'adam'] 'rmsprop', 'adam', 'adagrad', 'nadam', 'adadelta'\n",
    "batch_size = [64]\n",
    "# Simpler comparative options\n",
    "network_combinations = [(300, 2, 16, 0.2, 'huber_loss', 'adam', 64),\n",
    "                        (300, 2, 16, 0.2, 'huber_loss', 'adam', 64),\n",
    "                        ]\n",
    "\n",
    "import itertools\n",
    "# download all the different data\n",
    "stocks = {}\n",
    "for ticker in tickers:\n",
    "    print(ticker)\n",
    "    stocks[ticker] = getData(ticker, config[\"data\"][\"offset\"])\n",
    "print()\n",
    "\n",
    "# prepare data to be used for the different \n",
    "datasets = {}\n",
    "for opts in itertools.product(tickers, windows, lookaheads, validations, tests, scaled):\n",
    "    # print(opts)\n",
    "    # process the data with all of the settings\n",
    "    datasets[opts] = setupData(stocks[opts[0]], opts[1], opts[2], opts[3], opts[4], opts[5], config[\"data\"][\"scaler\"], config[\"data\"][\"inputs\"])\n",
    "    # datasets[opts] = 1024\n",
    "print()\n",
    "\n",
    "# find out how many total networks are being trained\n",
    "total = 0\n",
    "for dataset in datasets:\n",
    "    for opts in network_combinations:\n",
    "    # for opts in itertools.product(epochs, layers, num_neurons, dropouts, loss_functions, optimizers, batch_size):\n",
    "        total += 1\n",
    "\n",
    "# setup different models\n",
    "current = 0\n",
    "combinations = [] # setup a list with all the differnet networks\n",
    "for dataset in datasets:\n",
    "    # create all the different networks\n",
    "    print(dataset, dataset[1])\n",
    "    nets = {}\n",
    "    for idx, opts in enumerate(network_combinations):\n",
    "    # for opts in itertools.product(epochs, layers, num_neurons, dropouts, loss_functions, optimizers, batch_size):\n",
    "        current += 1\n",
    "        print(f'{current}/{total}', opts, opts[0])\n",
    "        # create a model (window, number_of_layers, neurons, ...)\n",
    "        model = generateModel(dataset[1], opts[1], opts[2], n_features= config[\"nn\"][\"nFeatures\"], dropout= opts[3], loss= opts[4], optimizer= opts[5], layerType= config[\"nn\"][\"layerType\"])\n",
    "        # train the model on the dataset (verbose has been set to 0)\n",
    "        history = model.fit(datasets[dataset][\"xtrain\"], datasets[dataset][\"ytrain\"], batch_size= opts[6], epochs= opts[0], validation_data=(datasets[dataset][\"xval\"], datasets[dataset][\"yval\"]), verbose=0)\n",
    "\n",
    "        train_history = pd.DataFrame(history.history)\n",
    "        fig = plt.figure(figsize=(25, 7), dpi=80)\n",
    "        plt.plot(range(1,opts[0]+1), train_history['loss'])\n",
    "        plt.plot(range(1,opts[0]+1), train_history['val_loss'])\n",
    "        # train_history.plot(figsize=(20,7))\n",
    "        plt.title('Training loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.xticks(range(10,opts[0]+1,10), rotation='vertical') \n",
    "        plt.grid(which=\"major\", color='k', linestyle=':', linewidth=1, alpha = 0.2)\n",
    "        plt.show()\n",
    "\n",
    "        # save the model\n",
    "        nets[idx, opts] = model\n",
    "    # store all the trained networks and their dataest in a dictionary\n",
    "    comb = {}\n",
    "    comb[\"dataset\"] = dataset # only save the name of the dataset used (it can be called from datasets at any point)\n",
    "    comb[\"networks\"] = nets\n",
    "    # add the combination to the combinations\n",
    "    combinations.append(comb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Ridge Regression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "def calculate_krr(_data, _scaled):\n",
    "    krr = KernelRidge(alpha=0.1, kernel='rbf') # why is rbf been used here? and what does the alpha control\n",
    "    # Train the model on the training data\n",
    "    krr.fit(_data['xtrain'][:, -1, :], _data['ytrain'])\n",
    "    # Make predictions on the test data\n",
    "    pred_krr = krr.predict(_data['xtest'][:, -1, :])\n",
    "    if _scaled:\n",
    "        pred_krr = np.squeeze(config[\"data\"][\"scaler\"].inverse_transform(np.expand_dims(pred_krr, axis=0)))\n",
    "    return pred_krr\n",
    "# Simple Moving Average\n",
    "def calculate_ma(_data, _scaled):\n",
    "    # calcualte the price using a simple moving average\n",
    "    pred_ma = np.mean(_data[\"xtest\"], axis = 1)\n",
    "    pred_ma = pred_ma[:, 0] #Â take only the adjclose price out\n",
    "    if _scaled:\n",
    "        pred_ma = np.squeeze(config[\"data\"][\"scaler\"].inverse_transform(np.expand_dims(pred_ma, axis=0)))\n",
    "    return pred_ma\n",
    "# Exponentila Moving Average\n",
    "def calculate_ema(_data, _scaled, _window):\n",
    "    # calculate the price using exponential moving average\n",
    "    pred_ema = _data[\"xtest\"][:, :, 0] # only take the \n",
    "    # pred_EMA = np.squeeze(config[\"data\"][\"scaler\"].inverse_transform(pred_EMA))\n",
    "    weights = np.exp(np.linspace(-1, 0, _window))\n",
    "    weights /= weights.sum()\n",
    "    pred_ema = pred_ema@weights\n",
    "    if _scaled:\n",
    "        pred_ema = np.squeeze(config[\"data\"][\"scaler\"].inverse_transform(np.expand_dims(pred_ema, axis=0)))\n",
    "    return pred_ema\n",
    "\n",
    "# GET RESULTS FROM ALL THE NETWORKS\n",
    "section = \"test\" #train, val, test\n",
    "# ^^^^^ COULD BE WORKED ON NEXT\n",
    "for comb in combinations:\n",
    "    data = datasets[comb[\"dataset\"]]\n",
    "    # copmare each network against each other\n",
    "    predictions = {}\n",
    "    y = data[\"ytest\"]\n",
    "\n",
    "    # if scaled\n",
    "    if comb['dataset'][5]:\n",
    "        y = np.squeeze(config[\"data\"][\"scaler\"].inverse_transform(np.expand_dims(y, axis=0)))\n",
    "# needs to be updated\n",
    "\n",
    "    for net in comb[\"networks\"]:\n",
    "        model = comb[\"networks\"][net]\n",
    "        # for the training\n",
    "        predictions[net] = model.predict(data[\"xtest\"])\n",
    "\n",
    "\n",
    "        # if the data is scaled\n",
    "        if comb['dataset'][5]:\n",
    "            predictions[net] = np.squeeze(config[\"data\"][\"scaler\"].inverse_transform(model.predict(data[\"xtest\"])))\n",
    " # needs to be updated\n",
    "\n",
    "    if 1:\n",
    "        # add the other methods to compare against\n",
    "        predictions[\"krr\"] = calculate_krr(data, comb['dataset'][5])\n",
    "        predictions[\"ma\"] = calculate_ma(data, comb['dataset'][5])\n",
    "        predictions[\"ema\"] = calculate_ema(data, comb['dataset'][5], comb[\"dataset\"][1])\n",
    "\n",
    "    comb[\"predictions\"] = predictions\n",
    "\n",
    "    # plot_predictions(data[\"testDates\"], y, predictions, section, comb['dataset'])\n",
    "    # calc_stats(y, predictions, comb[\"networks\"])\n",
    "    # store the predictions made\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# MAKE A FUNCTION FOR EACH COMPARISON TOOL\n",
    "# Mean Squared Error\n",
    "calc_mse = lambda y_true, y_pred: np.mean((y_true-y_pred)**2)\n",
    "# Root Mean Squared Error\n",
    "calc_rmse = lambda y_true, y_pred: np.sqrt(np.mean((y_true-y_pred)**2))\n",
    "# Mean Absolute Error\n",
    "calc_mae = lambda y_true, y_pred: np.mean(np.abs((y_true-y_pred)))\n",
    "# Mean Absolute Percentage Error\n",
    "calc_mape = lambda y_true, y_pred: np.mean(np.abs((y_true-y_pred) / y_true))*100\n",
    "# Directional accuracy\n",
    "calc_da = lambda y_true, y_pred: np.mean([(y_true[i+1]-y_true[i])*(y_pred[i+1]-y_pred[i]) > 0 for i in range(len(y_true)-1)])\n",
    "# Coefficient of determination RÂ²\n",
    "calc_cod = lambda y_true, y_pred: 1 - (sum((y_true - y_pred)**2) / sum((y_true - np.mean(y_true))**2))\n",
    "# calculate the statistics\n",
    "def calc_stats(_actual, _predictions, _nets = []):\n",
    "    d = {}\n",
    "    for net, pred in enumerate(_predictions):\n",
    "        prediction = _predictions[pred]\n",
    "        key = pred\n",
    "        if pred in _nets:\n",
    "            key = \"net\" + str(net)\n",
    "            print(key, \"-\", pred) # show the network config\n",
    "        # go throught the selected methods and get results\n",
    "        d[key] = [func(_actual, prediction) for func in (calc_mse, calc_rmse, calc_mae, calc_mape, calc_da, calc_cod)]\n",
    "    # create a dataframe from the information\n",
    "    df = pd.DataFrame(d)\n",
    "    df.index = ['MSE', 'RMSE', 'MAE', 'MAPE', 'DA', 'RÂ²'] # set the index of the dataframe\n",
    "    display(df) # display the dataframe\n",
    "    return df\n",
    "\n",
    "# to plot results\n",
    "def plot_predictions(_dates, _y, _predictions, _dataset, _section = 'Test', _highlight = []):\n",
    "    # plot the results\n",
    "    fig = plt.figure(figsize=(25, 7), dpi=80)\n",
    "    # plot the true values\n",
    "    plt.plot(_dates, _y, \"*-\", label = \"True Values\")\n",
    "    # plot all the predictions\n",
    "    for idx, pred in enumerate(_predictions):\n",
    "        if idx in _highlight:\n",
    "            alpha = 0.9\n",
    "        elif not _highlight: # if the list is empty\n",
    "            alpha = 0.4\n",
    "        else:\n",
    "            alpha = 0.1\n",
    "        plt.plot(_dates, _predictions[pred], \"*-\", label = pred, alpha = alpha)\n",
    "    plt.legend() # show the labels\n",
    "    plt.title(f\"{len(_y)} days of {_section.upper()} data {_dataset}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Closing price\")\n",
    "    plt.grid(which=\"major\", color='k', linestyle=':', linewidth=1, alpha = 0.2)\n",
    "    plt.show()\n",
    "\n",
    "# generate the file name to save all of the results\n",
    "file_name = datetime.now()\n",
    "file_name = str(file_name).replace(':', '_').split('.')\n",
    "file_name = 'training ' + file_name[0] + '.pkl'\n",
    "\n",
    "# Display the results\n",
    "for comb in combinations:\n",
    "    data = datasets[comb[\"dataset\"]]\n",
    "    y = data[\"ytest\"]\n",
    "\n",
    "    # if scaled\n",
    "    if comb['dataset'][5]:\n",
    "        y = np.squeeze(config[\"data\"][\"scaler\"].inverse_transform(np.expand_dims(y, axis=0)))\n",
    "# needs to be updated\n",
    "    predictions = comb[\"predictions\"]\n",
    "\n",
    "    plot_predictions(data[\"testDates\"], y, predictions, comb['dataset'], section, [7])\n",
    "    stats = calc_stats(y, predictions, comb[\"networks\"])\n",
    "\n",
    "    # save data sets with all the loading options\n",
    "    # with open(f\"results/{file_name.replace('training', str(comb['dataset']))}\", 'wb') as f:\n",
    "    #     pickle.dump([comb['dataset'], comb[\"predictions\"], data], f)\n",
    "    \n",
    "    # store the predictions made\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
