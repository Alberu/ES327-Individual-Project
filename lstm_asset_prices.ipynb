{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "To make sure that all the libraries have been installed, use the following command when in the virual enviroment\n",
    "\n",
    "pip install tensorflow pandas numpy matplotlib yahoo_fin sklearn scikit-learn\n",
    "NEED TO FIGURE OUT WHICH ONE OF THE TWO ACUTALLY WORKS, SECOND ONE SEEMS TO ACTUALLY WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole part of data processing needs to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next state the model is generated\n",
    "### Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, and create it usign the above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to create folders if they don't exist in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model (500 epoch took 210 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.0254\n",
      "Epoch 1: val_loss improved from inf to 0.00041, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 31s 323ms/step - loss: 0.0016 - mean_absolute_error: 0.0254 - val_loss: 4.1392e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 7.0346e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 2: val_loss improved from 0.00041 to 0.00034, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 23s 284ms/step - loss: 7.0346e-04 - mean_absolute_error: 0.0183 - val_loss: 3.3740e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 8.4472e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 3: val_loss did not improve from 0.00034\n",
      "80/80 [==============================] - 23s 284ms/step - loss: 8.4472e-04 - mean_absolute_error: 0.0203 - val_loss: 0.0018 - val_mean_absolute_error: 0.0438\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 7.5476e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 4: val_loss did not improve from 0.00034\n",
      "80/80 [==============================] - 23s 290ms/step - loss: 7.5476e-04 - mean_absolute_error: 0.0197 - val_loss: 3.9123e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.7869e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 5: val_loss did not improve from 0.00034\n",
      "80/80 [==============================] - 23s 290ms/step - loss: 6.7869e-04 - mean_absolute_error: 0.0178 - val_loss: 3.7825e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.2422e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 6: val_loss did not improve from 0.00034\n",
      "80/80 [==============================] - 24s 294ms/step - loss: 6.2422e-04 - mean_absolute_error: 0.0172 - val_loss: 3.5849e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 7.0736e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 7: val_loss did not improve from 0.00034\n",
      "80/80 [==============================] - 23s 293ms/step - loss: 7.0736e-04 - mean_absolute_error: 0.0183 - val_loss: 3.4080e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.9735e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 8: val_loss did not improve from 0.00034\n",
      "80/80 [==============================] - 23s 293ms/step - loss: 6.9735e-04 - mean_absolute_error: 0.0181 - val_loss: 3.4833e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.8472e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 9: val_loss improved from 0.00034 to 0.00032, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 23s 293ms/step - loss: 6.8472e-04 - mean_absolute_error: 0.0187 - val_loss: 3.1764e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.2284e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 10: val_loss did not improve from 0.00032\n",
      "80/80 [==============================] - 23s 292ms/step - loss: 6.2284e-04 - mean_absolute_error: 0.0176 - val_loss: 3.5456e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.9348e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 11: val_loss improved from 0.00032 to 0.00031, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 24s 296ms/step - loss: 5.9348e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1152e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 12/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.2974e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 12: val_loss did not improve from 0.00031\n",
      "80/80 [==============================] - 23s 286ms/step - loss: 6.2974e-04 - mean_absolute_error: 0.0168 - val_loss: 4.4403e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 13/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.4110e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 13: val_loss did not improve from 0.00031\n",
      "80/80 [==============================] - 23s 288ms/step - loss: 6.4110e-04 - mean_absolute_error: 0.0173 - val_loss: 0.0013 - val_mean_absolute_error: 0.0234\n",
      "Epoch 14/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.8760e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 14: val_loss improved from 0.00031 to 0.00031, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 23s 288ms/step - loss: 6.8760e-04 - mean_absolute_error: 0.0185 - val_loss: 3.0896e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 15/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.8727e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 15: val_loss did not improve from 0.00031\n",
      "80/80 [==============================] - 23s 286ms/step - loss: 5.8727e-04 - mean_absolute_error: 0.0170 - val_loss: 3.9248e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 16/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.5565e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 16: val_loss did not improve from 0.00031\n",
      "80/80 [==============================] - 23s 286ms/step - loss: 5.5565e-04 - mean_absolute_error: 0.0166 - val_loss: 3.7609e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 17/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.2889e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 17: val_loss improved from 0.00031 to 0.00030, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 23s 290ms/step - loss: 6.2889e-04 - mean_absolute_error: 0.0177 - val_loss: 3.0131e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 18/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.6911e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 18: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 287ms/step - loss: 5.6911e-04 - mean_absolute_error: 0.0167 - val_loss: 3.1724e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 19/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.1124e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 19: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 289ms/step - loss: 6.1124e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2341e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 20/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.9077e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 20: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 285ms/step - loss: 5.9077e-04 - mean_absolute_error: 0.0173 - val_loss: 7.1739e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 21/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.2529e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 21: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 287ms/step - loss: 6.2529e-04 - mean_absolute_error: 0.0178 - val_loss: 3.7885e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 22/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.9916e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 22: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 5.9916e-04 - mean_absolute_error: 0.0176 - val_loss: 3.6328e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 23/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.6151e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 23: val_loss improved from 0.00030 to 0.00030, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 5.6151e-04 - mean_absolute_error: 0.0172 - val_loss: 2.9681e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 24/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.3694e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 24: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 25s 311ms/step - loss: 5.3694e-04 - mean_absolute_error: 0.0168 - val_loss: 4.1309e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 25/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1370e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 25: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 5.1370e-04 - mean_absolute_error: 0.0162 - val_loss: 3.9394e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 26/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.7058e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 26: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 5.7058e-04 - mean_absolute_error: 0.0171 - val_loss: 3.6700e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 27/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.6110e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 27: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 289ms/step - loss: 5.6110e-04 - mean_absolute_error: 0.0172 - val_loss: 3.0827e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 28/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.5756e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 28: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 287ms/step - loss: 5.5756e-04 - mean_absolute_error: 0.0172 - val_loss: 5.8200e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 29/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.9389e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 29: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 292ms/step - loss: 5.9389e-04 - mean_absolute_error: 0.0182 - val_loss: 3.0936e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 30/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.6476e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 30: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 293ms/step - loss: 5.6476e-04 - mean_absolute_error: 0.0173 - val_loss: 3.0838e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 31/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.5286e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 31: val_loss improved from 0.00030 to 0.00030, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 23s 292ms/step - loss: 5.5286e-04 - mean_absolute_error: 0.0174 - val_loss: 2.9608e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 32/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.5712e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 32: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 24s 294ms/step - loss: 5.5712e-04 - mean_absolute_error: 0.0175 - val_loss: 3.0217e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 33/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.4022e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 33: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 293ms/step - loss: 5.4022e-04 - mean_absolute_error: 0.0170 - val_loss: 3.6853e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 34/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.4766e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 34: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 292ms/step - loss: 5.4766e-04 - mean_absolute_error: 0.0173 - val_loss: 5.4352e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 35/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.7522e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 35: val_loss improved from 0.00030 to 0.00030, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 23s 291ms/step - loss: 5.7522e-04 - mean_absolute_error: 0.0179 - val_loss: 2.9521e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 36/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.7272e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 36: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 288ms/step - loss: 5.7272e-04 - mean_absolute_error: 0.0175 - val_loss: 5.7677e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 37/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 6.0078e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 37: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 288ms/step - loss: 6.0078e-04 - mean_absolute_error: 0.0186 - val_loss: 4.7757e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 38/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.5056e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 38: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 23s 290ms/step - loss: 5.5056e-04 - mean_absolute_error: 0.0178 - val_loss: 3.1475e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 39/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.3332e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 39: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 24s 296ms/step - loss: 5.3332e-04 - mean_absolute_error: 0.0170 - val_loss: 3.0914e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 40/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9909e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 40: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 4.9909e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2545e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 41/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8010e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 41: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 4.8010e-04 - mean_absolute_error: 0.0168 - val_loss: 3.1039e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 42/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1013e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 42: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 25s 311ms/step - loss: 5.1013e-04 - mean_absolute_error: 0.0171 - val_loss: 3.0102e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 43/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.2889e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 43: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 25s 308ms/step - loss: 5.2889e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8617e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 44/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0351e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 44: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 5.0351e-04 - mean_absolute_error: 0.0172 - val_loss: 3.0299e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 45/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.7703e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 45: val_loss did not improve from 0.00030\n",
      "80/80 [==============================] - 24s 303ms/step - loss: 5.7703e-04 - mean_absolute_error: 0.0180 - val_loss: 4.2468e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 46/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1111e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 46: val_loss improved from 0.00030 to 0.00029, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 24s 303ms/step - loss: 5.1111e-04 - mean_absolute_error: 0.0175 - val_loss: 2.9268e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 47/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0089e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 47: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 5.0089e-04 - mean_absolute_error: 0.0170 - val_loss: 2.9755e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 48/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.2958e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 48: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 5.2958e-04 - mean_absolute_error: 0.0174 - val_loss: 3.0380e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 49/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1899e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 49: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 5.1899e-04 - mean_absolute_error: 0.0175 - val_loss: 3.1593e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 50/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8831e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 50: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 309ms/step - loss: 4.8831e-04 - mean_absolute_error: 0.0170 - val_loss: 2.9462e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 51/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9778e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 51: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 308ms/step - loss: 4.9778e-04 - mean_absolute_error: 0.0173 - val_loss: 4.7666e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 52/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.6487e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 52: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 5.6487e-04 - mean_absolute_error: 0.0182 - val_loss: 3.0227e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 53/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.5899e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 53: val_loss improved from 0.00029 to 0.00029, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 5.5899e-04 - mean_absolute_error: 0.0184 - val_loss: 2.9244e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 54/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9340e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 54: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 293ms/step - loss: 4.9340e-04 - mean_absolute_error: 0.0170 - val_loss: 2.9773e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 55/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7654e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 55: val_loss improved from 0.00029 to 0.00029, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 24s 296ms/step - loss: 4.7654e-04 - mean_absolute_error: 0.0172 - val_loss: 2.9125e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 56/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9910e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 56: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 297ms/step - loss: 4.9910e-04 - mean_absolute_error: 0.0174 - val_loss: 3.2827e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 57/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9458e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 57: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.9458e-04 - mean_absolute_error: 0.0172 - val_loss: 2.9395e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 58/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1264e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 58: val_loss improved from 0.00029 to 0.00029, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 5.1264e-04 - mean_absolute_error: 0.0174 - val_loss: 2.9116e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 59/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.2876e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 59: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 297ms/step - loss: 5.2876e-04 - mean_absolute_error: 0.0177 - val_loss: 3.5526e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 60/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1669e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 60: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 297ms/step - loss: 5.1669e-04 - mean_absolute_error: 0.0174 - val_loss: 3.5461e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 61/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.7676e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 61: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 5.7676e-04 - mean_absolute_error: 0.0188 - val_loss: 4.4284e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 62/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8505e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 62: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.8505e-04 - mean_absolute_error: 0.0174 - val_loss: 3.0439e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 63/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.5536e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 63: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 5.5536e-04 - mean_absolute_error: 0.0182 - val_loss: 3.3214e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 64/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1969e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 64: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 5.1969e-04 - mean_absolute_error: 0.0180 - val_loss: 3.7382e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 65/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8613e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 65: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 295ms/step - loss: 4.8613e-04 - mean_absolute_error: 0.0171 - val_loss: 3.4909e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 66/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1406e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 66: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 5.1406e-04 - mean_absolute_error: 0.0176 - val_loss: 2.9437e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 67/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8039e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 67: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.8039e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2322e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 68/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9553e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 68: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.9553e-04 - mean_absolute_error: 0.0174 - val_loss: 2.9257e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 69/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7293e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 69: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 4.7293e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1249e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 70/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6413e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 70: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 297ms/step - loss: 4.6413e-04 - mean_absolute_error: 0.0169 - val_loss: 2.9985e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 71/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0415e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 71: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 287ms/step - loss: 5.0415e-04 - mean_absolute_error: 0.0177 - val_loss: 4.5546e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 72/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5374e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 72: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 22s 281ms/step - loss: 4.5374e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1496e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 73/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8480e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 73: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 294ms/step - loss: 4.8480e-04 - mean_absolute_error: 0.0168 - val_loss: 2.9246e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 74/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8033e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 74: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.8033e-04 - mean_absolute_error: 0.0174 - val_loss: 4.1160e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 75/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.3531e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 75: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 5.3531e-04 - mean_absolute_error: 0.0179 - val_loss: 2.9158e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 76/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6820e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 76: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 297ms/step - loss: 4.6820e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3834e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 77/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8373e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 77: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 4.8373e-04 - mean_absolute_error: 0.0172 - val_loss: 3.4176e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 78/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7423e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 78: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.7423e-04 - mean_absolute_error: 0.0170 - val_loss: 2.9198e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 79/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8184e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 79: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.8184e-04 - mean_absolute_error: 0.0173 - val_loss: 3.9645e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 80/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0836e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 80: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 303ms/step - loss: 5.0836e-04 - mean_absolute_error: 0.0179 - val_loss: 2.9460e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 81/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6380e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 81: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.6380e-04 - mean_absolute_error: 0.0167 - val_loss: 2.9223e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 82/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0724e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 82: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 5.0724e-04 - mean_absolute_error: 0.0177 - val_loss: 3.3014e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 83/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7958e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 83: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.7958e-04 - mean_absolute_error: 0.0171 - val_loss: 3.5622e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 84/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1301e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 84: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 5.1301e-04 - mean_absolute_error: 0.0179 - val_loss: 3.1196e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 85/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6755e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 85: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.6755e-04 - mean_absolute_error: 0.0173 - val_loss: 3.6204e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 86/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0862e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 86: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 5.0862e-04 - mean_absolute_error: 0.0176 - val_loss: 3.1399e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 87/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8776e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 87: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 4.8776e-04 - mean_absolute_error: 0.0172 - val_loss: 3.1351e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 88/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7524e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 88: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 4.7524e-04 - mean_absolute_error: 0.0171 - val_loss: 3.7672e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 89/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9503e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 89: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 303ms/step - loss: 4.9503e-04 - mean_absolute_error: 0.0174 - val_loss: 3.4035e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 90/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4253e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 90: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 4.4253e-04 - mean_absolute_error: 0.0167 - val_loss: 3.0663e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 91/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6821e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 91: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 4.6821e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2592e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 92/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7136e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 92: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 4.7136e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2020e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 93/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6474e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 93: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 4.6474e-04 - mean_absolute_error: 0.0168 - val_loss: 2.9400e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 94/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8565e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 94: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 303ms/step - loss: 4.8565e-04 - mean_absolute_error: 0.0175 - val_loss: 3.1077e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 95/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6536e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 95: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 4.6536e-04 - mean_absolute_error: 0.0167 - val_loss: 4.7145e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 96/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8945e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 96: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 4.8945e-04 - mean_absolute_error: 0.0175 - val_loss: 3.4347e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 97/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7160e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 97: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 4.7160e-04 - mean_absolute_error: 0.0167 - val_loss: 3.0768e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 98/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9392e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 98: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 303ms/step - loss: 4.9392e-04 - mean_absolute_error: 0.0176 - val_loss: 3.8054e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 99/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8967e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 99: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 303ms/step - loss: 4.8967e-04 - mean_absolute_error: 0.0172 - val_loss: 3.4266e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 100/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7141e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 100: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 4.7141e-04 - mean_absolute_error: 0.0169 - val_loss: 3.0380e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 101/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9661e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 101: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 4.9661e-04 - mean_absolute_error: 0.0175 - val_loss: 3.0528e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 102/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0452e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 102: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 5.0452e-04 - mean_absolute_error: 0.0172 - val_loss: 3.1455e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 103/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6587e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 103: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 4.6587e-04 - mean_absolute_error: 0.0171 - val_loss: 3.0344e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 104/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.1578e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 104: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 5.1578e-04 - mean_absolute_error: 0.0184 - val_loss: 3.4017e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 105/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8077e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 105: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 286ms/step - loss: 4.8077e-04 - mean_absolute_error: 0.0171 - val_loss: 2.9787e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 106/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7519e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 106: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 282ms/step - loss: 4.7519e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3563e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 107/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3750e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 107: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 284ms/step - loss: 4.3750e-04 - mean_absolute_error: 0.0166 - val_loss: 4.0006e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 108/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7738e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 108: val_loss improved from 0.00029 to 0.00029, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 23s 288ms/step - loss: 4.7738e-04 - mean_absolute_error: 0.0174 - val_loss: 2.8934e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 109/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9349e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 109: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 286ms/step - loss: 4.9349e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0272e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 110/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0314e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 110: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 286ms/step - loss: 5.0314e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2380e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 111/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7388e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 111: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 287ms/step - loss: 4.7388e-04 - mean_absolute_error: 0.0174 - val_loss: 3.0407e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 112/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8739e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 112: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 290ms/step - loss: 4.8739e-04 - mean_absolute_error: 0.0175 - val_loss: 2.9843e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 113/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 5.0743e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 113: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 287ms/step - loss: 5.0743e-04 - mean_absolute_error: 0.0174 - val_loss: 2.9936e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 114/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7494e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 114: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 23s 288ms/step - loss: 4.7494e-04 - mean_absolute_error: 0.0174 - val_loss: 3.1811e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 115/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7837e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 115: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 4.7837e-04 - mean_absolute_error: 0.0173 - val_loss: 2.9697e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 116/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8174e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 116: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 4.8174e-04 - mean_absolute_error: 0.0173 - val_loss: 3.5056e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 117/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4595e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 117: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 4.4595e-04 - mean_absolute_error: 0.0165 - val_loss: 3.4590e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 118/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6659e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 118: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.6659e-04 - mean_absolute_error: 0.0168 - val_loss: 3.0931e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 119/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4071e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 119: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.4071e-04 - mean_absolute_error: 0.0162 - val_loss: 2.9211e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 120/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7942e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 120: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.7942e-04 - mean_absolute_error: 0.0165 - val_loss: 2.9059e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 121/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8472e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 121: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 4.8472e-04 - mean_absolute_error: 0.0172 - val_loss: 2.9623e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 122/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5625e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 122: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 4.5625e-04 - mean_absolute_error: 0.0167 - val_loss: 2.9101e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 123/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7933e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 123: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 4.7933e-04 - mean_absolute_error: 0.0173 - val_loss: 3.2087e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 124/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5139e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 124: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 4.5139e-04 - mean_absolute_error: 0.0168 - val_loss: 3.2761e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 125/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5558e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 125: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 4.5558e-04 - mean_absolute_error: 0.0169 - val_loss: 3.0460e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 126/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4766e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 126: val_loss improved from 0.00029 to 0.00029, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 24s 306ms/step - loss: 4.4766e-04 - mean_absolute_error: 0.0165 - val_loss: 2.8593e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 127/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8349e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 127: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 308ms/step - loss: 4.8349e-04 - mean_absolute_error: 0.0171 - val_loss: 2.9134e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 128/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5890e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 128: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 4.5890e-04 - mean_absolute_error: 0.0165 - val_loss: 3.1086e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 129/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9405e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 129: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 4.9405e-04 - mean_absolute_error: 0.0175 - val_loss: 3.1203e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 130/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5954e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 130: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 4.5954e-04 - mean_absolute_error: 0.0168 - val_loss: 2.8817e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 131/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5656e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 131: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 4.5656e-04 - mean_absolute_error: 0.0168 - val_loss: 3.2625e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 132/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5841e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 132: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 309ms/step - loss: 4.5841e-04 - mean_absolute_error: 0.0165 - val_loss: 2.9224e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 133/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5719e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 133: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 311ms/step - loss: 4.5719e-04 - mean_absolute_error: 0.0164 - val_loss: 3.1151e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 134/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6590e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 134: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 4.6590e-04 - mean_absolute_error: 0.0166 - val_loss: 3.9688e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 135/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5776e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 135: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 4.5776e-04 - mean_absolute_error: 0.0166 - val_loss: 2.9466e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 136/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5964e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 136: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 4.5964e-04 - mean_absolute_error: 0.0165 - val_loss: 2.8816e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 137/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8892e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 137: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 4.8892e-04 - mean_absolute_error: 0.0171 - val_loss: 2.9770e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 138/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5445e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 138: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 4.5445e-04 - mean_absolute_error: 0.0165 - val_loss: 3.3642e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 139/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9939e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 139: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 4.9939e-04 - mean_absolute_error: 0.0172 - val_loss: 3.0117e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 140/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5215e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 140: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 306ms/step - loss: 4.5215e-04 - mean_absolute_error: 0.0164 - val_loss: 3.6621e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 141/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6371e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 141: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 309ms/step - loss: 4.6371e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3110e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 142/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6793e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 142: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 4.6793e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3768e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 143/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4909e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 143: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 4.4909e-04 - mean_absolute_error: 0.0164 - val_loss: 2.9695e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 144/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5167e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 144: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 4.5167e-04 - mean_absolute_error: 0.0165 - val_loss: 2.9462e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 145/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6139e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 145: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 305ms/step - loss: 4.6139e-04 - mean_absolute_error: 0.0164 - val_loss: 2.9776e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 146/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6142e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 146: val_loss did not improve from 0.00029\n",
      "80/80 [==============================] - 24s 297ms/step - loss: 4.6142e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2040e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 147/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7334e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 147: val_loss improved from 0.00029 to 0.00028, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 24s 295ms/step - loss: 4.7334e-04 - mean_absolute_error: 0.0168 - val_loss: 2.8257e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 148/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6829e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 148: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 23s 291ms/step - loss: 4.6829e-04 - mean_absolute_error: 0.0170 - val_loss: 4.5734e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 149/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6726e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 149: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 23s 291ms/step - loss: 4.6726e-04 - mean_absolute_error: 0.0169 - val_loss: 2.8794e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 150/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3432e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 150: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 23s 290ms/step - loss: 4.3432e-04 - mean_absolute_error: 0.0163 - val_loss: 3.0661e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 151/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5184e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 151: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 297ms/step - loss: 4.5184e-04 - mean_absolute_error: 0.0164 - val_loss: 4.5943e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 152/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5040e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 152: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 4.5040e-04 - mean_absolute_error: 0.0166 - val_loss: 2.9153e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 153/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7844e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 153: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 308ms/step - loss: 4.7844e-04 - mean_absolute_error: 0.0170 - val_loss: 2.9415e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 154/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7184e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 154: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 4.7184e-04 - mean_absolute_error: 0.0169 - val_loss: 2.9761e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 155/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4804e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 155: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 308ms/step - loss: 4.4804e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6983e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 156/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5195e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 156: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 4.5195e-04 - mean_absolute_error: 0.0165 - val_loss: 3.0266e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 157/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5551e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 157: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 308ms/step - loss: 4.5551e-04 - mean_absolute_error: 0.0163 - val_loss: 2.9056e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 158/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5355e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 158: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 4.5355e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2503e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 159/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6985e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 159: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 4.6985e-04 - mean_absolute_error: 0.0166 - val_loss: 2.8478e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 160/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4842e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 160: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 309ms/step - loss: 4.4842e-04 - mean_absolute_error: 0.0164 - val_loss: 3.3647e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 161/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5881e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 161: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 4.5881e-04 - mean_absolute_error: 0.0165 - val_loss: 3.1684e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 162/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3772e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 162: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 4.3772e-04 - mean_absolute_error: 0.0162 - val_loss: 2.9612e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 163/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9359e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 163: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 4.9359e-04 - mean_absolute_error: 0.0172 - val_loss: 3.8108e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 164/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5851e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 164: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 4.5851e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6625e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 165/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6852e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 165: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 4.6852e-04 - mean_absolute_error: 0.0169 - val_loss: 3.4003e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 166/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4352e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 166: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.4352e-04 - mean_absolute_error: 0.0163 - val_loss: 2.8551e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 167/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4806e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 167: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.4806e-04 - mean_absolute_error: 0.0163 - val_loss: 2.9828e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 168/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6490e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 168: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.6490e-04 - mean_absolute_error: 0.0166 - val_loss: 3.1809e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 169/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6356e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 169: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.6356e-04 - mean_absolute_error: 0.0166 - val_loss: 4.5973e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 170/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6970e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 170: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 294ms/step - loss: 4.6970e-04 - mean_absolute_error: 0.0166 - val_loss: 4.1887e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 171/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4205e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 171: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 23s 293ms/step - loss: 4.4205e-04 - mean_absolute_error: 0.0163 - val_loss: 3.0148e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 172/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5324e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 172: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 296ms/step - loss: 4.5324e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2815e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 173/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6986e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 173: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.6986e-04 - mean_absolute_error: 0.0172 - val_loss: 3.6631e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 174/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5775e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 174: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.5775e-04 - mean_absolute_error: 0.0172 - val_loss: 3.0466e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 175/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4527e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 175: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.4527e-04 - mean_absolute_error: 0.0163 - val_loss: 2.9806e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 176/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4518e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 176: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.4518e-04 - mean_absolute_error: 0.0163 - val_loss: 2.8540e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 177/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3389e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 177: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.3389e-04 - mean_absolute_error: 0.0161 - val_loss: 4.1377e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 178/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7677e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 178: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.7677e-04 - mean_absolute_error: 0.0166 - val_loss: 4.6543e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 179/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5815e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 179: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.5815e-04 - mean_absolute_error: 0.0166 - val_loss: 2.8329e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 180/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4258e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 180: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.4258e-04 - mean_absolute_error: 0.0163 - val_loss: 2.9150e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 181/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6507e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 181: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 298ms/step - loss: 4.6507e-04 - mean_absolute_error: 0.0168 - val_loss: 2.8612e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 182/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5866e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 182: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 299ms/step - loss: 4.5866e-04 - mean_absolute_error: 0.0164 - val_loss: 3.2995e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 183/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5658e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 183: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.5658e-04 - mean_absolute_error: 0.0165 - val_loss: 5.2832e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 184/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.8198e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 184: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 4.8198e-04 - mean_absolute_error: 0.0172 - val_loss: 2.9169e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 185/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6234e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 185: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 300ms/step - loss: 4.6234e-04 - mean_absolute_error: 0.0165 - val_loss: 3.3905e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 186/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5920e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 186: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 297ms/step - loss: 4.5920e-04 - mean_absolute_error: 0.0167 - val_loss: 3.0829e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 187/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6636e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 187: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 4.6636e-04 - mean_absolute_error: 0.0169 - val_loss: 3.4239e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 188/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4358e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 188: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 4.4358e-04 - mean_absolute_error: 0.0163 - val_loss: 2.8298e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 189/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4405e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 189: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 4.4405e-04 - mean_absolute_error: 0.0162 - val_loss: 2.8620e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 190/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4004e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 190: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 4.4004e-04 - mean_absolute_error: 0.0162 - val_loss: 2.8637e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 191/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6891e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 191: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 318ms/step - loss: 4.6891e-04 - mean_absolute_error: 0.0165 - val_loss: 3.0643e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 192/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.9218e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 192: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 4.9218e-04 - mean_absolute_error: 0.0170 - val_loss: 2.8535e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 193/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2780e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 193: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.2780e-04 - mean_absolute_error: 0.0161 - val_loss: 3.0135e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 194/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2095e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 194: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 319ms/step - loss: 4.2095e-04 - mean_absolute_error: 0.0160 - val_loss: 3.0221e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 195/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4062e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 195: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 319ms/step - loss: 4.4062e-04 - mean_absolute_error: 0.0162 - val_loss: 2.8932e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 196/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7192e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 196: val_loss improved from 0.00028 to 0.00028, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.7192e-04 - mean_absolute_error: 0.0164 - val_loss: 2.8195e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 197/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5031e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 197: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 4.5031e-04 - mean_absolute_error: 0.0166 - val_loss: 2.9710e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 198/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2948e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 198: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 318ms/step - loss: 4.2948e-04 - mean_absolute_error: 0.0162 - val_loss: 2.8988e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 199/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7284e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 199: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.7284e-04 - mean_absolute_error: 0.0168 - val_loss: 3.0258e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 200/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3227e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 200: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 4.3227e-04 - mean_absolute_error: 0.0162 - val_loss: 2.8416e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 201/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2824e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 201: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.2824e-04 - mean_absolute_error: 0.0162 - val_loss: 3.1592e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 202/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2811e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 202: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 4.2811e-04 - mean_absolute_error: 0.0161 - val_loss: 2.9473e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 203/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4279e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 203: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.4279e-04 - mean_absolute_error: 0.0165 - val_loss: 2.9634e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 204/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3867e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 204: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 318ms/step - loss: 4.3867e-04 - mean_absolute_error: 0.0163 - val_loss: 2.8669e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 205/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3289e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 205: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 324ms/step - loss: 4.3289e-04 - mean_absolute_error: 0.0160 - val_loss: 3.1494e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 206/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3004e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 206: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 4.3004e-04 - mean_absolute_error: 0.0161 - val_loss: 2.8227e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 207/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1953e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 207: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 4.1953e-04 - mean_absolute_error: 0.0162 - val_loss: 3.1000e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 208/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5012e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 208: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 301ms/step - loss: 4.5012e-04 - mean_absolute_error: 0.0166 - val_loss: 2.8920e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 209/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5210e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 209: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 302ms/step - loss: 4.5210e-04 - mean_absolute_error: 0.0163 - val_loss: 3.5848e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 210/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4088e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 210: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 303ms/step - loss: 4.4088e-04 - mean_absolute_error: 0.0165 - val_loss: 3.3316e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 211/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6157e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 211: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 4.6157e-04 - mean_absolute_error: 0.0168 - val_loss: 3.2986e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 212/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4364e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 212: val_loss improved from 0.00028 to 0.00028, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 4.4364e-04 - mean_absolute_error: 0.0166 - val_loss: 2.8193e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 213/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4585e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 213: val_loss improved from 0.00028 to 0.00028, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 311ms/step - loss: 4.4585e-04 - mean_absolute_error: 0.0162 - val_loss: 2.7898e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 214/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4845e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 214: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 311ms/step - loss: 4.4845e-04 - mean_absolute_error: 0.0161 - val_loss: 2.8875e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 215/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2102e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 215: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 4.2102e-04 - mean_absolute_error: 0.0160 - val_loss: 2.8622e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 216/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3928e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 216: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 4.3928e-04 - mean_absolute_error: 0.0160 - val_loss: 3.0047e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 217/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4660e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 217: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 4.4660e-04 - mean_absolute_error: 0.0165 - val_loss: 2.8070e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 218/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3364e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 218: val_loss improved from 0.00028 to 0.00028, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 308ms/step - loss: 4.3364e-04 - mean_absolute_error: 0.0160 - val_loss: 2.7677e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 219/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3191e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 219: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 319ms/step - loss: 4.3191e-04 - mean_absolute_error: 0.0161 - val_loss: 3.2424e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 220/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3413e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 220: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 24s 304ms/step - loss: 4.3413e-04 - mean_absolute_error: 0.0160 - val_loss: 2.9096e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 221/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6356e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 221: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 4.6356e-04 - mean_absolute_error: 0.0166 - val_loss: 3.3236e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 222/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3951e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 222: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 339ms/step - loss: 4.3951e-04 - mean_absolute_error: 0.0161 - val_loss: 3.0606e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 223/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4381e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 223: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 32s 396ms/step - loss: 4.4381e-04 - mean_absolute_error: 0.0161 - val_loss: 2.9569e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 224/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2110e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 224: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 4.2110e-04 - mean_absolute_error: 0.0156 - val_loss: 2.9441e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 225/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6848e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 225: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 4.6848e-04 - mean_absolute_error: 0.0164 - val_loss: 3.0584e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 226/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5500e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 226: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 4.5500e-04 - mean_absolute_error: 0.0164 - val_loss: 2.8359e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 227/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1908e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 227: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 341ms/step - loss: 4.1908e-04 - mean_absolute_error: 0.0160 - val_loss: 2.7742e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 228/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4978e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 228: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 4.4978e-04 - mean_absolute_error: 0.0164 - val_loss: 3.3906e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 229/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6315e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 229: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.6315e-04 - mean_absolute_error: 0.0166 - val_loss: 2.8004e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 230/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5796e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 230: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 28s 353ms/step - loss: 4.5796e-04 - mean_absolute_error: 0.0162 - val_loss: 3.2484e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 231/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6462e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 231: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 28s 350ms/step - loss: 4.6462e-04 - mean_absolute_error: 0.0168 - val_loss: 2.8425e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 232/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6220e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 232: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 29s 366ms/step - loss: 4.6220e-04 - mean_absolute_error: 0.0167 - val_loss: 3.4926e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 233/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2411e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 233: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 28s 350ms/step - loss: 4.2411e-04 - mean_absolute_error: 0.0162 - val_loss: 3.0967e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 234/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3947e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 234: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 337ms/step - loss: 4.3947e-04 - mean_absolute_error: 0.0163 - val_loss: 2.7779e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 235/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1721e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 235: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 332ms/step - loss: 4.1721e-04 - mean_absolute_error: 0.0158 - val_loss: 2.8829e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 236/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1444e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 236: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 338ms/step - loss: 4.1444e-04 - mean_absolute_error: 0.0157 - val_loss: 2.8133e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 237/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6264e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 237: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 337ms/step - loss: 4.6264e-04 - mean_absolute_error: 0.0168 - val_loss: 3.0007e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 238/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1986e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 238: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 336ms/step - loss: 4.1986e-04 - mean_absolute_error: 0.0156 - val_loss: 2.7867e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 239/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4297e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 239: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 4.4297e-04 - mean_absolute_error: 0.0162 - val_loss: 4.5098e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 240/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3656e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 240: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 4.3656e-04 - mean_absolute_error: 0.0162 - val_loss: 3.5015e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 241/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0876e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 241: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 4.0876e-04 - mean_absolute_error: 0.0155 - val_loss: 3.0712e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 242/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4043e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 242: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 4.4043e-04 - mean_absolute_error: 0.0161 - val_loss: 2.9191e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 243/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2466e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 243: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 4.2466e-04 - mean_absolute_error: 0.0163 - val_loss: 2.8256e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 244/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3580e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 244: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 331ms/step - loss: 4.3580e-04 - mean_absolute_error: 0.0161 - val_loss: 3.0268e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 245/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3155e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 245: val_loss improved from 0.00028 to 0.00028, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 27s 341ms/step - loss: 4.3155e-04 - mean_absolute_error: 0.0161 - val_loss: 2.7644e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 246/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5416e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 246: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 28s 349ms/step - loss: 4.5416e-04 - mean_absolute_error: 0.0167 - val_loss: 2.8169e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 247/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.6196e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 247: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 4.6196e-04 - mean_absolute_error: 0.0166 - val_loss: 2.8766e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 248/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3103e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 248: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 326ms/step - loss: 4.3103e-04 - mean_absolute_error: 0.0159 - val_loss: 2.9478e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 249/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4455e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 249: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 333ms/step - loss: 4.4455e-04 - mean_absolute_error: 0.0162 - val_loss: 2.8906e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 250/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3423e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 250: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 28s 349ms/step - loss: 4.3423e-04 - mean_absolute_error: 0.0161 - val_loss: 2.9436e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 251/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3787e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 251: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 29s 359ms/step - loss: 4.3787e-04 - mean_absolute_error: 0.0161 - val_loss: 2.8787e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 252/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4195e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 252: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 29s 358ms/step - loss: 4.4195e-04 - mean_absolute_error: 0.0161 - val_loss: 2.9137e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 253/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5119e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 253: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 29s 363ms/step - loss: 4.5119e-04 - mean_absolute_error: 0.0163 - val_loss: 2.8981e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 254/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4038e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 254: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 31s 391ms/step - loss: 4.4038e-04 - mean_absolute_error: 0.0164 - val_loss: 2.9744e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 255/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1982e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 255: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 29s 368ms/step - loss: 4.1982e-04 - mean_absolute_error: 0.0159 - val_loss: 2.9115e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 256/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2651e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 256: val_loss improved from 0.00028 to 0.00028, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 29s 367ms/step - loss: 4.2651e-04 - mean_absolute_error: 0.0160 - val_loss: 2.7627e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 257/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2025e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 257: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 342ms/step - loss: 4.2025e-04 - mean_absolute_error: 0.0160 - val_loss: 3.0661e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 258/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3252e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 258: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 4.3252e-04 - mean_absolute_error: 0.0160 - val_loss: 2.8453e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 259/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2554e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 259: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 331ms/step - loss: 4.2554e-04 - mean_absolute_error: 0.0162 - val_loss: 3.2018e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 260/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7336e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 260: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 4.7336e-04 - mean_absolute_error: 0.0167 - val_loss: 2.9237e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 261/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2666e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 261: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 4.2666e-04 - mean_absolute_error: 0.0159 - val_loss: 3.2259e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 262/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1424e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 262: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 4.1424e-04 - mean_absolute_error: 0.0160 - val_loss: 2.8499e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 263/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1249e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 263: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 334ms/step - loss: 4.1249e-04 - mean_absolute_error: 0.0157 - val_loss: 2.8462e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 264/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1805e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 264: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 337ms/step - loss: 4.1805e-04 - mean_absolute_error: 0.0158 - val_loss: 2.8735e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 265/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1344e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 265: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 328ms/step - loss: 4.1344e-04 - mean_absolute_error: 0.0157 - val_loss: 2.8824e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 266/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1012e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 266: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 336ms/step - loss: 4.1012e-04 - mean_absolute_error: 0.0158 - val_loss: 2.8037e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 267/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1469e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 267: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 28s 349ms/step - loss: 4.1469e-04 - mean_absolute_error: 0.0160 - val_loss: 2.8182e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 268/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3111e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 268: val_loss improved from 0.00028 to 0.00028, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 27s 335ms/step - loss: 4.3111e-04 - mean_absolute_error: 0.0159 - val_loss: 2.7554e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 269/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2077e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 269: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 4.2077e-04 - mean_absolute_error: 0.0158 - val_loss: 2.8977e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 270/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3311e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 270: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 4.3311e-04 - mean_absolute_error: 0.0160 - val_loss: 2.9323e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 271/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3192e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 271: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 4.3192e-04 - mean_absolute_error: 0.0158 - val_loss: 2.8885e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 272/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4081e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 272: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 4.4081e-04 - mean_absolute_error: 0.0163 - val_loss: 3.3073e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 273/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2629e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 273: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 319ms/step - loss: 4.2629e-04 - mean_absolute_error: 0.0160 - val_loss: 2.7712e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 274/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3786e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 274: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 309ms/step - loss: 4.3786e-04 - mean_absolute_error: 0.0162 - val_loss: 3.6533e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 275/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2723e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 275: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 4.2723e-04 - mean_absolute_error: 0.0161 - val_loss: 3.1389e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 276/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.5379e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 276: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 4.5379e-04 - mean_absolute_error: 0.0162 - val_loss: 2.8170e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 277/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1943e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 277: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 4.1943e-04 - mean_absolute_error: 0.0157 - val_loss: 3.2812e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 278/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2466e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 278: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 25s 311ms/step - loss: 4.2466e-04 - mean_absolute_error: 0.0161 - val_loss: 2.7787e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 279/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2232e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 279: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 4.2232e-04 - mean_absolute_error: 0.0157 - val_loss: 3.1874e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 280/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3894e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 280: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.3894e-04 - mean_absolute_error: 0.0163 - val_loss: 2.8400e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 281/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0231e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 281: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 4.0231e-04 - mean_absolute_error: 0.0155 - val_loss: 3.0393e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 282/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1905e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 282: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 27s 335ms/step - loss: 4.1905e-04 - mean_absolute_error: 0.0157 - val_loss: 2.7798e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 283/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9577e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 283: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 3.9577e-04 - mean_absolute_error: 0.0155 - val_loss: 2.8240e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 284/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2718e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 284: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 4.2718e-04 - mean_absolute_error: 0.0157 - val_loss: 3.3810e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 285/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3134e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 285: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 324ms/step - loss: 4.3134e-04 - mean_absolute_error: 0.0161 - val_loss: 2.8050e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 286/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1516e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 286: val_loss did not improve from 0.00028\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 4.1516e-04 - mean_absolute_error: 0.0159 - val_loss: 2.9392e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 287/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2354e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 287: val_loss improved from 0.00028 to 0.00027, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 4.2354e-04 - mean_absolute_error: 0.0160 - val_loss: 2.7360e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 288/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2078e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 288: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 4.2078e-04 - mean_absolute_error: 0.0159 - val_loss: 2.9480e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 289/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1417e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 289: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 328ms/step - loss: 4.1417e-04 - mean_absolute_error: 0.0162 - val_loss: 3.2558e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 290/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1331e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 290: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.1331e-04 - mean_absolute_error: 0.0160 - val_loss: 2.7609e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 291/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2114e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 291: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 4.2114e-04 - mean_absolute_error: 0.0159 - val_loss: 2.7543e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 292/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1141e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 292: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 4.1141e-04 - mean_absolute_error: 0.0158 - val_loss: 2.8023e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 293/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0199e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 293: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 4.0199e-04 - mean_absolute_error: 0.0157 - val_loss: 3.2157e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 294/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2475e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 294: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 4.2475e-04 - mean_absolute_error: 0.0160 - val_loss: 2.9241e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 295/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4175e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 295: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 4.4175e-04 - mean_absolute_error: 0.0161 - val_loss: 2.7471e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 296/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2247e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 296: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 4.2247e-04 - mean_absolute_error: 0.0158 - val_loss: 2.9567e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 297/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9128e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 297: val_loss improved from 0.00027 to 0.00027, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 3.9128e-04 - mean_absolute_error: 0.0158 - val_loss: 2.7012e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 298/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2907e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 298: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 328ms/step - loss: 4.2907e-04 - mean_absolute_error: 0.0160 - val_loss: 3.2513e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 299/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0383e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 299: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 4.0383e-04 - mean_absolute_error: 0.0155 - val_loss: 2.8186e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 300/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1193e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 300: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 27s 333ms/step - loss: 4.1193e-04 - mean_absolute_error: 0.0161 - val_loss: 2.7153e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 301/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0277e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 301: val_loss improved from 0.00027 to 0.00027, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 4.0277e-04 - mean_absolute_error: 0.0155 - val_loss: 2.6834e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 302/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.2731e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 302: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 4.2731e-04 - mean_absolute_error: 0.0159 - val_loss: 2.7312e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 303/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0559e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 303: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 28s 356ms/step - loss: 4.0559e-04 - mean_absolute_error: 0.0158 - val_loss: 2.7364e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 304/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1019e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 304: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 27s 337ms/step - loss: 4.1019e-04 - mean_absolute_error: 0.0157 - val_loss: 2.8413e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 305/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9454e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 305: val_loss did not improve from 0.00027\n",
      "80/80 [==============================] - 29s 361ms/step - loss: 3.9454e-04 - mean_absolute_error: 0.0154 - val_loss: 2.8987e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 306/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3102e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 306: val_loss improved from 0.00027 to 0.00027, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 28s 349ms/step - loss: 4.3102e-04 - mean_absolute_error: 0.0161 - val_loss: 2.6723e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 307/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8646e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 307: val_loss improved from 0.00027 to 0.00026, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 29s 362ms/step - loss: 3.8646e-04 - mean_absolute_error: 0.0155 - val_loss: 2.5741e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 308/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3355e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 308: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 28s 345ms/step - loss: 4.3355e-04 - mean_absolute_error: 0.0166 - val_loss: 2.7450e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 309/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0614e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 309: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 28s 351ms/step - loss: 4.0614e-04 - mean_absolute_error: 0.0158 - val_loss: 3.1320e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 310/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1287e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 310: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 28s 349ms/step - loss: 4.1287e-04 - mean_absolute_error: 0.0158 - val_loss: 2.9677e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 311/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1290e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 311: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 4.1290e-04 - mean_absolute_error: 0.0158 - val_loss: 3.1383e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 312/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8990e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 312: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 331ms/step - loss: 3.8990e-04 - mean_absolute_error: 0.0156 - val_loss: 2.6125e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 313/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1051e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 313: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 326ms/step - loss: 4.1051e-04 - mean_absolute_error: 0.0158 - val_loss: 2.7062e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 314/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4675e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 314: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 326ms/step - loss: 4.4675e-04 - mean_absolute_error: 0.0162 - val_loss: 2.5986e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 315/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1153e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 315: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 328ms/step - loss: 4.1153e-04 - mean_absolute_error: 0.0160 - val_loss: 2.9798e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 316/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3432e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 316: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 4.3432e-04 - mean_absolute_error: 0.0160 - val_loss: 2.9420e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 317/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0737e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 317: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 4.0737e-04 - mean_absolute_error: 0.0159 - val_loss: 2.8175e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 318/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0185e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 318: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 27s 335ms/step - loss: 4.0185e-04 - mean_absolute_error: 0.0156 - val_loss: 2.6668e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 319/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.4507e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 319: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 27s 337ms/step - loss: 4.4507e-04 - mean_absolute_error: 0.0162 - val_loss: 2.6707e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 320/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9437e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 320: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 27s 334ms/step - loss: 3.9437e-04 - mean_absolute_error: 0.0157 - val_loss: 2.5910e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 321/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8824e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 321: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 3.8824e-04 - mean_absolute_error: 0.0157 - val_loss: 2.9771e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 322/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.6844e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 322: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 27s 344ms/step - loss: 3.6844e-04 - mean_absolute_error: 0.0153 - val_loss: 2.7823e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 323/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9877e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 323: val_loss did not improve from 0.00026\n",
      "80/80 [==============================] - 26s 324ms/step - loss: 3.9877e-04 - mean_absolute_error: 0.0156 - val_loss: 2.9535e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 324/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1679e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 324: val_loss improved from 0.00026 to 0.00025, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 326ms/step - loss: 4.1679e-04 - mean_absolute_error: 0.0163 - val_loss: 2.4644e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 325/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0542e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 325: val_loss did not improve from 0.00025\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 4.0542e-04 - mean_absolute_error: 0.0159 - val_loss: 2.5951e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 326/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9329e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 326: val_loss improved from 0.00025 to 0.00025, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 328ms/step - loss: 3.9329e-04 - mean_absolute_error: 0.0155 - val_loss: 2.4504e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 327/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.7061e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 327: val_loss did not improve from 0.00025\n",
      "80/80 [==============================] - 28s 354ms/step - loss: 3.7061e-04 - mean_absolute_error: 0.0152 - val_loss: 2.4951e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 328/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8786e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 328: val_loss did not improve from 0.00025\n",
      "80/80 [==============================] - 27s 341ms/step - loss: 3.8786e-04 - mean_absolute_error: 0.0155 - val_loss: 2.5828e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 329/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8911e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 329: val_loss did not improve from 0.00025\n",
      "80/80 [==============================] - 27s 335ms/step - loss: 3.8911e-04 - mean_absolute_error: 0.0154 - val_loss: 2.6014e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 330/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8971e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 330: val_loss did not improve from 0.00025\n",
      "80/80 [==============================] - 27s 340ms/step - loss: 3.8971e-04 - mean_absolute_error: 0.0155 - val_loss: 3.1407e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 331/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.7460e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 331: val_loss improved from 0.00025 to 0.00024, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 27s 336ms/step - loss: 3.7460e-04 - mean_absolute_error: 0.0156 - val_loss: 2.3711e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 332/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9280e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 332: val_loss did not improve from 0.00024\n",
      "80/80 [==============================] - 28s 347ms/step - loss: 3.9280e-04 - mean_absolute_error: 0.0155 - val_loss: 2.5639e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 333/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8395e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 333: val_loss did not improve from 0.00024\n",
      "80/80 [==============================] - 26s 331ms/step - loss: 3.8395e-04 - mean_absolute_error: 0.0154 - val_loss: 2.6982e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 334/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.7961e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 334: val_loss improved from 0.00024 to 0.00023, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 3.7961e-04 - mean_absolute_error: 0.0157 - val_loss: 2.3358e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 335/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9368e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 335: val_loss did not improve from 0.00023\n",
      "80/80 [==============================] - 27s 334ms/step - loss: 3.9368e-04 - mean_absolute_error: 0.0156 - val_loss: 2.3982e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 336/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.6223e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 336: val_loss improved from 0.00023 to 0.00022, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 28s 345ms/step - loss: 3.6223e-04 - mean_absolute_error: 0.0150 - val_loss: 2.2327e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 337/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5130e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 337: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 336ms/step - loss: 3.5130e-04 - mean_absolute_error: 0.0148 - val_loss: 2.3460e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 338/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5206e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 338: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 338ms/step - loss: 3.5206e-04 - mean_absolute_error: 0.0150 - val_loss: 2.2396e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 339/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5503e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 339: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 3.5503e-04 - mean_absolute_error: 0.0153 - val_loss: 2.7018e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 340/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.0595e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 340: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 334ms/step - loss: 4.0595e-04 - mean_absolute_error: 0.0156 - val_loss: 2.5838e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 341/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1651e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 341: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 342ms/step - loss: 4.1651e-04 - mean_absolute_error: 0.0157 - val_loss: 2.7087e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 342/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1913e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 342: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 340ms/step - loss: 4.1913e-04 - mean_absolute_error: 0.0160 - val_loss: 4.0295e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 343/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.3143e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 343: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 29s 363ms/step - loss: 4.3143e-04 - mean_absolute_error: 0.0162 - val_loss: 2.4202e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 344/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9637e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 344: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 28s 351ms/step - loss: 3.9637e-04 - mean_absolute_error: 0.0156 - val_loss: 3.5547e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 345/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8205e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 345: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 3.8205e-04 - mean_absolute_error: 0.0155 - val_loss: 2.5237e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 346/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5766e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 346: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 3.5766e-04 - mean_absolute_error: 0.0152 - val_loss: 2.6248e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 347/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.6436e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 347: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 328ms/step - loss: 3.6436e-04 - mean_absolute_error: 0.0152 - val_loss: 2.3193e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 348/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.9270e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 348: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 3.9270e-04 - mean_absolute_error: 0.0156 - val_loss: 2.8219e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 349/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.6423e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 349: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 336ms/step - loss: 3.6423e-04 - mean_absolute_error: 0.0155 - val_loss: 2.3109e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 350/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4606e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 350: val_loss improved from 0.00022 to 0.00022, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 27s 332ms/step - loss: 3.4606e-04 - mean_absolute_error: 0.0150 - val_loss: 2.1689e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 351/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.7808e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 351: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 338ms/step - loss: 3.7808e-04 - mean_absolute_error: 0.0156 - val_loss: 2.2936e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 352/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2483e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 352: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 343ms/step - loss: 3.2483e-04 - mean_absolute_error: 0.0144 - val_loss: 2.2915e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 353/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.6795e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 353: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 3.6795e-04 - mean_absolute_error: 0.0154 - val_loss: 2.3162e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 354/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3450e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 354: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 27s 334ms/step - loss: 3.3450e-04 - mean_absolute_error: 0.0147 - val_loss: 2.6726e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 355/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8245e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 355: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 326ms/step - loss: 3.8245e-04 - mean_absolute_error: 0.0153 - val_loss: 2.8657e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 356/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8374e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 356: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 3.8374e-04 - mean_absolute_error: 0.0153 - val_loss: 2.4802e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 357/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3156e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 357: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 3.3156e-04 - mean_absolute_error: 0.0148 - val_loss: 2.5479e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 358/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2435e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 358: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 28s 345ms/step - loss: 3.2435e-04 - mean_absolute_error: 0.0146 - val_loss: 2.9398e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 359/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.7606e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 359: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 3.7606e-04 - mean_absolute_error: 0.0153 - val_loss: 3.4113e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 360/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.8058e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 360: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 3.8058e-04 - mean_absolute_error: 0.0154 - val_loss: 2.3177e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 361/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4790e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 361: val_loss did not improve from 0.00022\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 3.4790e-04 - mean_absolute_error: 0.0150 - val_loss: 2.1985e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 362/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4379e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 362: val_loss improved from 0.00022 to 0.00021, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 3.4379e-04 - mean_absolute_error: 0.0149 - val_loss: 2.1477e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 363/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4740e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 363: val_loss did not improve from 0.00021\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 3.4740e-04 - mean_absolute_error: 0.0149 - val_loss: 2.4057e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 364/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5537e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 364: val_loss did not improve from 0.00021\n",
      "80/80 [==============================] - 26s 319ms/step - loss: 3.5537e-04 - mean_absolute_error: 0.0154 - val_loss: 2.2339e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 365/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3568e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 365: val_loss did not improve from 0.00021\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 3.3568e-04 - mean_absolute_error: 0.0147 - val_loss: 2.8583e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 366/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3441e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 366: val_loss did not improve from 0.00021\n",
      "80/80 [==============================] - 25s 318ms/step - loss: 3.3441e-04 - mean_absolute_error: 0.0149 - val_loss: 2.1946e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 367/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2644e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 367: val_loss did not improve from 0.00021\n",
      "80/80 [==============================] - 25s 307ms/step - loss: 3.2644e-04 - mean_absolute_error: 0.0147 - val_loss: 2.4547e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 368/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4421e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 368: val_loss improved from 0.00021 to 0.00021, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 3.4421e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0959e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 369/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3284e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 369: val_loss did not improve from 0.00021\n",
      "80/80 [==============================] - 26s 319ms/step - loss: 3.3284e-04 - mean_absolute_error: 0.0147 - val_loss: 2.1093e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 370/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4233e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 370: val_loss did not improve from 0.00021\n",
      "80/80 [==============================] - 26s 324ms/step - loss: 3.4233e-04 - mean_absolute_error: 0.0150 - val_loss: 2.7850e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 371/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.7020e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 371: val_loss did not improve from 0.00021\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 3.7020e-04 - mean_absolute_error: 0.0151 - val_loss: 2.2469e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 372/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.6776e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 372: val_loss improved from 0.00021 to 0.00020, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 3.6776e-04 - mean_absolute_error: 0.0154 - val_loss: 1.9898e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 373/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.9952e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 373: val_loss improved from 0.00020 to 0.00019, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 2.9952e-04 - mean_absolute_error: 0.0143 - val_loss: 1.9149e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 374/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3051e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 374: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 3.3051e-04 - mean_absolute_error: 0.0144 - val_loss: 3.4324e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 375/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.7378e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 375: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 326ms/step - loss: 4.7378e-04 - mean_absolute_error: 0.0163 - val_loss: 2.5572e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 376/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.7291e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 376: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 319ms/step - loss: 3.7291e-04 - mean_absolute_error: 0.0153 - val_loss: 2.1574e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 377/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.7278e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 377: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 3.7278e-04 - mean_absolute_error: 0.0153 - val_loss: 3.1213e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 378/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 4.1351e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 378: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 4.1351e-04 - mean_absolute_error: 0.0159 - val_loss: 2.8187e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 379/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.6797e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 379: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 3.6797e-04 - mean_absolute_error: 0.0153 - val_loss: 2.1342e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 380/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3815e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 380: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 3.3815e-04 - mean_absolute_error: 0.0149 - val_loss: 2.3155e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 381/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0878e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 381: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 3.0878e-04 - mean_absolute_error: 0.0144 - val_loss: 2.0341e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 382/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5265e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 382: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 3.5265e-04 - mean_absolute_error: 0.0150 - val_loss: 2.2413e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 383/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5555e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 383: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 3.5555e-04 - mean_absolute_error: 0.0150 - val_loss: 2.5170e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 384/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5562e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 384: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 326ms/step - loss: 3.5562e-04 - mean_absolute_error: 0.0151 - val_loss: 2.2275e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 385/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3607e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 385: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 3.3607e-04 - mean_absolute_error: 0.0148 - val_loss: 2.1747e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 386/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5062e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 386: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 328ms/step - loss: 3.5062e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0980e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 387/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4034e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 387: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 3.4034e-04 - mean_absolute_error: 0.0149 - val_loss: 2.7071e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 388/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4307e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 388: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 3.4307e-04 - mean_absolute_error: 0.0150 - val_loss: 1.9255e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 389/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2250e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 389: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 27s 341ms/step - loss: 3.2250e-04 - mean_absolute_error: 0.0144 - val_loss: 2.4726e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 390/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3551e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 390: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 27s 344ms/step - loss: 3.3551e-04 - mean_absolute_error: 0.0147 - val_loss: 2.3123e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 391/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3681e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 391: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 27s 341ms/step - loss: 3.3681e-04 - mean_absolute_error: 0.0147 - val_loss: 2.1644e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 392/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3734e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 392: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 3.3734e-04 - mean_absolute_error: 0.0146 - val_loss: 2.0890e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 393/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3685e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 393: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 3.3685e-04 - mean_absolute_error: 0.0148 - val_loss: 2.2045e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 394/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.5424e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 394: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 27s 332ms/step - loss: 3.5424e-04 - mean_absolute_error: 0.0151 - val_loss: 2.5396e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 395/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2521e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 395: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 3.2521e-04 - mean_absolute_error: 0.0148 - val_loss: 2.3283e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 396/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.9655e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 396: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 2.9655e-04 - mean_absolute_error: 0.0142 - val_loss: 1.9268e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 397/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.1482e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 397: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 3.1482e-04 - mean_absolute_error: 0.0146 - val_loss: 1.9503e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 398/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3514e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 398: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 25s 309ms/step - loss: 3.3514e-04 - mean_absolute_error: 0.0148 - val_loss: 1.9893e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 399/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.1220e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 399: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 3.1220e-04 - mean_absolute_error: 0.0144 - val_loss: 2.0568e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 400/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2519e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 400: val_loss did not improve from 0.00019\n",
      "80/80 [==============================] - 25s 309ms/step - loss: 3.2519e-04 - mean_absolute_error: 0.0145 - val_loss: 1.9803e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 401/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2051e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 401: val_loss improved from 0.00019 to 0.00018, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 3.2051e-04 - mean_absolute_error: 0.0146 - val_loss: 1.8393e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 402/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0620e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 402: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 3.0620e-04 - mean_absolute_error: 0.0142 - val_loss: 2.8012e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 403/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4174e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 403: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 3.4174e-04 - mean_absolute_error: 0.0150 - val_loss: 2.1346e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 404/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.4776e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 404: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 3.4776e-04 - mean_absolute_error: 0.0149 - val_loss: 2.0991e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 405/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.1093e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 405: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 3.1093e-04 - mean_absolute_error: 0.0145 - val_loss: 2.1582e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 406/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2842e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 406: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 27s 335ms/step - loss: 3.2842e-04 - mean_absolute_error: 0.0144 - val_loss: 1.9688e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 407/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.9181e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 407: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 2.9181e-04 - mean_absolute_error: 0.0141 - val_loss: 1.9527e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 408/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.1904e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 408: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 3.1904e-04 - mean_absolute_error: 0.0144 - val_loss: 1.8947e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 409/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2782e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 409: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 3.2782e-04 - mean_absolute_error: 0.0147 - val_loss: 2.2151e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 410/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3819e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 410: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 3.3819e-04 - mean_absolute_error: 0.0149 - val_loss: 2.4577e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 411/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2718e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 411: val_loss did not improve from 0.00018\n",
      "80/80 [==============================] - 25s 310ms/step - loss: 3.2718e-04 - mean_absolute_error: 0.0149 - val_loss: 2.1795e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 412/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8700e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 412: val_loss improved from 0.00018 to 0.00017, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 2.8700e-04 - mean_absolute_error: 0.0143 - val_loss: 1.7323e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 413/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8636e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 413: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 309ms/step - loss: 2.8636e-04 - mean_absolute_error: 0.0140 - val_loss: 1.8537e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 414/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.9753e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 414: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.9753e-04 - mean_absolute_error: 0.0144 - val_loss: 1.8114e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 415/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7576e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 415: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.7576e-04 - mean_absolute_error: 0.0137 - val_loss: 1.8167e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 416/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.2245e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 416: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 3.2245e-04 - mean_absolute_error: 0.0145 - val_loss: 2.3783e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 417/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8974e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 417: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.8974e-04 - mean_absolute_error: 0.0139 - val_loss: 2.1154e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 418/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0258e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 418: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 3.0258e-04 - mean_absolute_error: 0.0143 - val_loss: 1.8077e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 419/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8765e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 419: val_loss improved from 0.00017 to 0.00017, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.8765e-04 - mean_absolute_error: 0.0140 - val_loss: 1.7285e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 420/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0905e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 420: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 3.0905e-04 - mean_absolute_error: 0.0145 - val_loss: 2.0161e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 421/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.9677e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 421: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.9677e-04 - mean_absolute_error: 0.0141 - val_loss: 1.9453e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 422/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0852e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 422: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 3.0852e-04 - mean_absolute_error: 0.0142 - val_loss: 1.8315e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 423/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0490e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 423: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 3.0490e-04 - mean_absolute_error: 0.0142 - val_loss: 2.3498e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 424/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0085e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 424: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 3.0085e-04 - mean_absolute_error: 0.0143 - val_loss: 1.8580e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 425/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8509e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 425: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.8509e-04 - mean_absolute_error: 0.0140 - val_loss: 2.1709e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 426/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8161e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 426: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.8161e-04 - mean_absolute_error: 0.0141 - val_loss: 1.8892e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 427/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6729e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 427: val_loss improved from 0.00017 to 0.00017, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.6729e-04 - mean_absolute_error: 0.0138 - val_loss: 1.6632e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 428/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8354e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 428: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.8354e-04 - mean_absolute_error: 0.0138 - val_loss: 1.8821e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 429/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8197e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 429: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.8197e-04 - mean_absolute_error: 0.0138 - val_loss: 1.7539e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 430/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.1180e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 430: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 3.1180e-04 - mean_absolute_error: 0.0143 - val_loss: 1.9672e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 431/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8731e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 431: val_loss did not improve from 0.00017\n",
      "80/80 [==============================] - 26s 328ms/step - loss: 2.8731e-04 - mean_absolute_error: 0.0139 - val_loss: 1.9379e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 432/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7778e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 432: val_loss improved from 0.00017 to 0.00016, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 2.7778e-04 - mean_absolute_error: 0.0139 - val_loss: 1.5688e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 433/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0742e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 433: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 3.0742e-04 - mean_absolute_error: 0.0144 - val_loss: 2.2306e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 434/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8107e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 434: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 2.8107e-04 - mean_absolute_error: 0.0138 - val_loss: 3.0456e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 435/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.3808e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 435: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 3.3808e-04 - mean_absolute_error: 0.0148 - val_loss: 2.0933e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 436/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6287e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 436: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 2.6287e-04 - mean_absolute_error: 0.0136 - val_loss: 1.7234e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 437/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8387e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 437: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 319ms/step - loss: 2.8387e-04 - mean_absolute_error: 0.0140 - val_loss: 1.9935e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 438/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7031e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 438: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 2.7031e-04 - mean_absolute_error: 0.0139 - val_loss: 1.6653e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 439/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8697e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 439: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 319ms/step - loss: 2.8697e-04 - mean_absolute_error: 0.0139 - val_loss: 1.8968e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 440/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0033e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 440: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 320ms/step - loss: 3.0033e-04 - mean_absolute_error: 0.0140 - val_loss: 2.7056e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 441/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.1159e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 441: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 3.1159e-04 - mean_absolute_error: 0.0143 - val_loss: 2.1303e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 442/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7308e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 442: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 331ms/step - loss: 2.7308e-04 - mean_absolute_error: 0.0138 - val_loss: 2.4021e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 443/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7062e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 443: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 324ms/step - loss: 2.7062e-04 - mean_absolute_error: 0.0137 - val_loss: 1.6977e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 444/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8380e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 444: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 2.8380e-04 - mean_absolute_error: 0.0139 - val_loss: 1.7699e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 445/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 3.0433e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 445: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 321ms/step - loss: 3.0433e-04 - mean_absolute_error: 0.0140 - val_loss: 1.8256e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 446/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7631e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 446: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 26s 326ms/step - loss: 2.7631e-04 - mean_absolute_error: 0.0139 - val_loss: 1.6840e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 447/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7559e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 447: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 27s 339ms/step - loss: 2.7559e-04 - mean_absolute_error: 0.0138 - val_loss: 1.6317e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 448/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8582e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 448: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.8582e-04 - mean_absolute_error: 0.0140 - val_loss: 1.8915e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 449/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7543e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 449: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.7543e-04 - mean_absolute_error: 0.0138 - val_loss: 1.6968e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 450/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6389e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 450: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.6389e-04 - mean_absolute_error: 0.0135 - val_loss: 1.8205e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 451/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7137e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 451: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.7137e-04 - mean_absolute_error: 0.0137 - val_loss: 1.8345e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 452/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7488e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 452: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.7488e-04 - mean_absolute_error: 0.0138 - val_loss: 2.0049e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 453/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7330e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 453: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.7330e-04 - mean_absolute_error: 0.0137 - val_loss: 1.9174e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 454/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6741e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 454: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.6741e-04 - mean_absolute_error: 0.0135 - val_loss: 1.7415e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 455/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4435e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 455: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.4435e-04 - mean_absolute_error: 0.0132 - val_loss: 1.7324e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 456/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7881e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 456: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.7881e-04 - mean_absolute_error: 0.0139 - val_loss: 1.8608e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 457/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7571e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 457: val_loss did not improve from 0.00016\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.7571e-04 - mean_absolute_error: 0.0139 - val_loss: 2.1813e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 458/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7412e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 458: val_loss improved from 0.00016 to 0.00015, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.7412e-04 - mean_absolute_error: 0.0138 - val_loss: 1.5300e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 459/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7242e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 459: val_loss did not improve from 0.00015\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.7242e-04 - mean_absolute_error: 0.0138 - val_loss: 1.6624e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 460/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7845e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 460: val_loss did not improve from 0.00015\n",
      "80/80 [==============================] - 26s 330ms/step - loss: 2.7845e-04 - mean_absolute_error: 0.0137 - val_loss: 1.8376e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 461/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7632e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 461: val_loss improved from 0.00015 to 0.00015, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 332ms/step - loss: 2.7632e-04 - mean_absolute_error: 0.0138 - val_loss: 1.4582e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 462/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8242e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 462: val_loss did not improve from 0.00015\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 2.8242e-04 - mean_absolute_error: 0.0137 - val_loss: 1.8160e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 463/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7509e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 463: val_loss did not improve from 0.00015\n",
      "80/80 [==============================] - 26s 329ms/step - loss: 2.7509e-04 - mean_absolute_error: 0.0139 - val_loss: 1.6708e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 464/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8444e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 464: val_loss did not improve from 0.00015\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.8444e-04 - mean_absolute_error: 0.0138 - val_loss: 1.5844e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 465/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8163e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 465: val_loss did not improve from 0.00015\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.8163e-04 - mean_absolute_error: 0.0140 - val_loss: 1.5087e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 466/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.5646e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 466: val_loss improved from 0.00015 to 0.00014, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.5646e-04 - mean_absolute_error: 0.0134 - val_loss: 1.4102e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 467/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7837e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 467: val_loss did not improve from 0.00014\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.7837e-04 - mean_absolute_error: 0.0138 - val_loss: 1.4996e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 468/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.5729e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 468: val_loss did not improve from 0.00014\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.5729e-04 - mean_absolute_error: 0.0133 - val_loss: 1.5062e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 469/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6379e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 469: val_loss did not improve from 0.00014\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.6379e-04 - mean_absolute_error: 0.0135 - val_loss: 1.4259e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 470/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4768e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 470: val_loss did not improve from 0.00014\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.4768e-04 - mean_absolute_error: 0.0133 - val_loss: 1.7945e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 471/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7935e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 471: val_loss did not improve from 0.00014\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.7935e-04 - mean_absolute_error: 0.0139 - val_loss: 1.6379e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 472/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6851e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 472: val_loss did not improve from 0.00014\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.6851e-04 - mean_absolute_error: 0.0137 - val_loss: 1.5362e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 473/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.3794e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 473: val_loss improved from 0.00014 to 0.00013, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 26s 325ms/step - loss: 2.3794e-04 - mean_absolute_error: 0.0131 - val_loss: 1.2914e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 474/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.5967e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 474: val_loss did not improve from 0.00013\n",
      "80/80 [==============================] - 27s 337ms/step - loss: 2.5967e-04 - mean_absolute_error: 0.0134 - val_loss: 1.3808e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 475/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7791e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 475: val_loss did not improve from 0.00013\n",
      "80/80 [==============================] - 26s 327ms/step - loss: 2.7791e-04 - mean_absolute_error: 0.0137 - val_loss: 1.5805e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 476/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.5615e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 476: val_loss did not improve from 0.00013\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 2.5615e-04 - mean_absolute_error: 0.0133 - val_loss: 1.4779e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 477/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4169e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 477: val_loss improved from 0.00013 to 0.00012, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.4169e-04 - mean_absolute_error: 0.0130 - val_loss: 1.2343e-04 - val_mean_absolute_error: 0.0074\n",
      "Epoch 478/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4684e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 478: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.4684e-04 - mean_absolute_error: 0.0131 - val_loss: 1.6011e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 479/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6042e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 479: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 2.6042e-04 - mean_absolute_error: 0.0135 - val_loss: 1.5785e-04 - val_mean_absolute_error: 0.0092\n",
      "Epoch 480/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8161e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 480: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 312ms/step - loss: 2.8161e-04 - mean_absolute_error: 0.0139 - val_loss: 1.4340e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 481/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6121e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 481: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 318ms/step - loss: 2.6121e-04 - mean_absolute_error: 0.0134 - val_loss: 1.2751e-04 - val_mean_absolute_error: 0.0077\n",
      "Epoch 482/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8100e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 482: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.8100e-04 - mean_absolute_error: 0.0137 - val_loss: 1.3783e-04 - val_mean_absolute_error: 0.0080\n",
      "Epoch 483/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.3110e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 483: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.3110e-04 - mean_absolute_error: 0.0131 - val_loss: 1.5759e-04 - val_mean_absolute_error: 0.0081\n",
      "Epoch 484/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.5863e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 484: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 26s 323ms/step - loss: 2.5863e-04 - mean_absolute_error: 0.0133 - val_loss: 1.3025e-04 - val_mean_absolute_error: 0.0081\n",
      "Epoch 485/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.3899e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 485: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 26s 322ms/step - loss: 2.3899e-04 - mean_absolute_error: 0.0132 - val_loss: 1.3267e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 486/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.3748e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 486: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 26s 324ms/step - loss: 2.3748e-04 - mean_absolute_error: 0.0131 - val_loss: 1.3422e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 487/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.7241e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 487: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 313ms/step - loss: 2.7241e-04 - mean_absolute_error: 0.0138 - val_loss: 1.8047e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 488/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4543e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 488: val_loss improved from 0.00012 to 0.00012, saving model to results\\2022-11-22_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.4543e-04 - mean_absolute_error: 0.0133 - val_loss: 1.1736e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 489/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4660e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 489: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.4660e-04 - mean_absolute_error: 0.0132 - val_loss: 1.2562e-04 - val_mean_absolute_error: 0.0075\n",
      "Epoch 490/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6174e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 490: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.6174e-04 - mean_absolute_error: 0.0135 - val_loss: 1.2817e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 491/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6557e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 491: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.6557e-04 - mean_absolute_error: 0.0133 - val_loss: 1.7642e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 492/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.8614e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 492: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 314ms/step - loss: 2.8614e-04 - mean_absolute_error: 0.0140 - val_loss: 1.3418e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 493/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.5111e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 493: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 2.5111e-04 - mean_absolute_error: 0.0134 - val_loss: 1.4134e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 494/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4715e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 494: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 2.4715e-04 - mean_absolute_error: 0.0132 - val_loss: 1.2991e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 495/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4620e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 495: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.4620e-04 - mean_absolute_error: 0.0133 - val_loss: 1.2641e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 496/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4649e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 496: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.4649e-04 - mean_absolute_error: 0.0134 - val_loss: 1.4889e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 497/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.5263e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 497: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 318ms/step - loss: 2.5263e-04 - mean_absolute_error: 0.0132 - val_loss: 1.1888e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 498/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.3738e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 498: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.3738e-04 - mean_absolute_error: 0.0131 - val_loss: 1.2688e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 499/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.4979e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 499: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 2.4979e-04 - mean_absolute_error: 0.0133 - val_loss: 1.3881e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 500/500\n",
      "80/80 [==============================] - ETA: 0s - loss: 2.6108e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 500: val_loss did not improve from 0.00012\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 2.6108e-04 - mean_absolute_error: 0.0134 - val_loss: 1.2892e-04 - val_mean_absolute_error: 0.0078\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 4s 75ms/step\n"
     ]
    }
   ],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 98.06$\n",
      "huber_loss loss: 0.00011735848966054618\n",
      "Mean Absolute Error: 1.6014822275943057\n",
      "Accuracy score: 0.631578947368421\n",
      "Total buy profit: 822.1001628935337\n",
      "Total sell profit: 720.5994968414307\n",
      "Total profit: 1542.6996597349644\n",
      "Profit per trade: 1.2118614766181968\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  open        high         low       close    adjclose  \\\n",
      "2022-08-08  142.050003  144.229996  138.289993  139.410004  139.410004   \n",
      "2022-09-02  129.500000  131.380005  126.389999  127.510002  127.510002   \n",
      "2022-09-09  130.910004  133.690002  130.759995  133.270004  133.270004   \n",
      "2022-09-14  127.360001  128.839996  126.330002  128.550003  128.550003   \n",
      "2022-09-20  123.349998  124.400002  121.139999  122.190002  122.190002   \n",
      "2022-09-21  122.489998  123.760002  118.449997  118.540001  118.540001   \n",
      "2022-09-22  117.080002  118.790001  116.260002  117.309998  117.309998   \n",
      "2022-10-13  107.879997  113.440002  105.349998  112.529999  112.529999   \n",
      "2022-10-19  114.709999  116.589996  113.220001  115.070000  115.070000   \n",
      "2022-10-24  119.980003  120.389999  116.570000  119.820000  119.820000   \n",
      "\n",
      "              volume ticker  adjclose_15  true_adjclose_15  buy_profit  \\\n",
      "2022-08-08  52229000   AMZN   139.257156        129.789993         0.0   \n",
      "2022-09-02  57429800   AMZN   117.360191        115.150002         0.0   \n",
      "2022-09-09  49387600   AMZN   118.409866        113.000000         0.0   \n",
      "2022-09-14  45316800   AMZN   117.343979        120.949997         0.0   \n",
      "2022-09-20  47698400   AMZN   110.180733        112.209999         0.0   \n",
      "2022-09-21  58498900   AMZN   108.757736        112.900002         0.0   \n",
      "2022-09-22  55229200   AMZN   107.294159        112.529999         0.0   \n",
      "2022-10-13  86868100   AMZN    99.644295         89.300003         0.0   \n",
      "2022-10-19  47198100   AMZN   101.213608         86.139999         0.0   \n",
      "2022-10-24  49531500   AMZN   103.763802         98.489998         0.0   \n",
      "\n",
      "            sell_profit  \n",
      "2022-08-08     9.620010  \n",
      "2022-09-02    12.360001  \n",
      "2022-09-09    20.270004  \n",
      "2022-09-14     7.600006  \n",
      "2022-09-20     9.980003  \n",
      "2022-09-21     5.639999  \n",
      "2022-09-22     4.779999  \n",
      "2022-10-13    23.229996  \n",
      "2022-10-19    28.930000  \n",
      "2022-10-24    21.330002  \n"
     ]
    }
   ],
   "source": [
    "print(final_df.tail(10))\n",
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 50, 256)           268288    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 256)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 793,857\n",
      "Trainable params: 793,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "training = history\n",
    "# plot\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,3))\n",
    "       \n",
    "## training    \n",
    "ax[0].set(title=\"Training\")    \n",
    "ax11 = ax[0].twinx()    \n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "\n",
    "ax[0].set_xlabel('Epochs')\n",
    "\n",
    "ax[0].set_ylabel('Loss', color='black')    \n",
    "for metric in metrics:        \n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "    ax11.set_ylabel(\"Score\", color='steelblue')    \n",
    "ax11.legend()\n",
    "        \n",
    "## validation    \n",
    "ax[1].set(title=\"Validation\")    \n",
    "ax22 = ax[1].twinx()    \n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "\n",
    "ax[1].set_xlabel('Epochs')\n",
    "\n",
    "ax[1].set_ylabel('Loss', color='black')    \n",
    "for metric in metrics:          \n",
    "    ax22.plot(training.history['val_'+metric], label=metric)\n",
    "    ax22.set_ylabel(\"Score\", color=\"steelblue\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a81014e9fecf489d837e149a008146a4c9c51ab693fe93d5722a2af8b8e8cdae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
